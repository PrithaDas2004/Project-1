# Project-1
This project, developed by a 5-member team, focuses on Environmental Sound Classification using the UrbanSound8K dataset. We classified various sounds—such as sirens, dog barks, and rain—into predefined categories using a technique called feature fusion, where we combined three types of audio features: MFCCs, spectrograms, and scalograms. These features, which are visual representations of sound, were processed using AlexNet, a deep convolutional neural network, to extract patterns and classify the sounds. Our feature fusion approach achieved an accuracy of approximately 97%, significantly outperforming the use of individual features alone (MFCC: 89%, Spectrogram: 92%, Scalogram: 89%), demonstrating the effectiveness of our method for challenging sound classification tasks.
